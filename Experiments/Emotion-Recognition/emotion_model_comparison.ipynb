{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Emotion Recognition Model Comparison\n",
                "\n",
                "This notebook tests the emotion models used in the VocalMind pipeline:\n",
                "- **Text**: `j-hartmann/emotion-english-distilroberta-base` (7 classes)\n",
                "- **Audio**: `audeering/wav2vec2-large-robust-12-ft-emotion-msp-dim` (dimensional → mapped to 7 classes)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Device: CUDA\n"
                    ]
                }
            ],
            "source": [
                "import os\n",
                "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
                "import warnings\n",
                "warnings.filterwarnings(\"ignore\")\n",
                "\n",
                "import torch\n",
                "import numpy as np\n",
                "import librosa\n",
                "from transformers import pipeline, Wav2Vec2Processor\n",
                "\n",
                "device = 0 if torch.cuda.is_available() else -1\n",
                "print(f\"Device: {'CUDA' if device == 0 else 'CPU'}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Text Emotion Model: `j-hartmann/emotion-english-distilroberta-base`\n",
                "\n",
                "**Classes (7):** anger, disgust, fear, joy, neutral, sadness, surprise"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 26,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loaded: j-hartmann/emotion-english-distilroberta-base\n"
                    ]
                }
            ],
            "source": [
                "text_emotion_model = \"j-hartmann/emotion-english-distilroberta-base\"\n",
                "text_classifier = pipeline(\"text-classification\", model=text_emotion_model, device=device, top_k=None)\n",
                "print(f\"Loaded: {text_emotion_model}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 27,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Text Emotion Classification Results:\n",
                        "\n",
                        "'I'm so happy today, everything is going great!...' -> JOY (97.2%)\n",
                        "'I've been charged twice. This is unacceptable....' -> ANGER (82.3%)\n",
                        "'I'm so sorry about that. Let me look into it....' -> SADNESS (90.8%)\n",
                        "'Really? A credit too? That's amazing!...' -> SURPRISE (95.3%)\n",
                        "'The meeting is scheduled for 3 PM tomorrow....' -> NEUTRAL (70.4%)\n"
                    ]
                }
            ],
            "source": [
                "# Test sentences\n",
                "test_sentences = [\n",
                "    \"I'm so happy today, everything is going great!\",\n",
                "    \"I've been charged twice. This is unacceptable.\",\n",
                "    \"I'm so sorry about that. Let me look into it.\",\n",
                "    \"Really? A credit too? That's amazing!\",\n",
                "    \"The meeting is scheduled for 3 PM tomorrow.\"\n",
                "]\n",
                "\n",
                "print(\"Text Emotion Classification Results:\\n\")\n",
                "for sentence in test_sentences:\n",
                "    result = text_classifier(sentence)[0]\n",
                "    top = max(result, key=lambda x: x['score'])\n",
                "    print(f\"'{sentence[:50]}...' -> {top['label'].upper()} ({top['score']:.1%})\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Audio Emotion Model: `audeering/wav2vec2-large-robust-12-ft-emotion-msp-dim`\n",
                "\n",
                "**Output**: Dimensional values (arousal, dominance, valence) on 0-1 scale\n",
                "\n",
                "Trained on MSP-Podcast dataset (natural conversational speech)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 28,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Some weights of EmotionModel were not initialized from the model checkpoint at audeering/wav2vec2-large-robust-12-ft-emotion-msp-dim and are newly initialized: ['classifier.bias', 'classifier.weight', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
                        "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loaded: audeering/wav2vec2-large-robust-12-ft-emotion-msp-dim\n"
                    ]
                }
            ],
            "source": [
                "import torch.nn as nn\n",
                "from transformers import Wav2Vec2Model, Wav2Vec2PreTrainedModel\n",
                "\n",
                "class EmotionModel(Wav2Vec2PreTrainedModel):\n",
                "    \"\"\"Custom model wrapper for audeering's dimensional emotion model.\"\"\"\n",
                "    def __init__(self, config):\n",
                "        super().__init__(config)\n",
                "        self.wav2vec2 = Wav2Vec2Model(config)\n",
                "        self.classifier = nn.Linear(config.hidden_size, 3)  # arousal, dominance, valence\n",
                "        self.init_weights()\n",
                "    \n",
                "    def forward(self, input_values):\n",
                "        outputs = self.wav2vec2(input_values)\n",
                "        hidden_states = outputs.last_hidden_state\n",
                "        pooled = hidden_states.mean(dim=1)\n",
                "        logits = self.classifier(pooled)\n",
                "        return hidden_states, logits\n",
                "\n",
                "audio_model_name = \"audeering/wav2vec2-large-robust-12-ft-emotion-msp-dim\"\n",
                "processor = Wav2Vec2Processor.from_pretrained(audio_model_name)\n",
                "audio_model = EmotionModel.from_pretrained(audio_model_name)\n",
                "audio_model.eval()\n",
                "if torch.cuda.is_available():\n",
                "    audio_model = audio_model.cuda()\n",
                "print(f\"Loaded: {audio_model_name}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 29,
            "metadata": {},
            "outputs": [],
            "source": [
                "def dimensions_to_emotion(arousal, valence, dominance):\n",
                "    \"\"\"Map dimensional values to emotion category.\"\"\"\n",
                "    if arousal > 0.7:\n",
                "        if valence > 0.5:\n",
                "            return \"joy\"\n",
                "        else:\n",
                "            return \"anger\" if dominance > 0.5 else \"fear\"\n",
                "    if valence > 0.6:\n",
                "        return \"joy\"\n",
                "    if valence < 0.4:\n",
                "        if arousal > 0.6:\n",
                "            return \"anger\" if dominance > 0.5 else \"fear\"\n",
                "        elif arousal < 0.4:\n",
                "            return \"sadness\"\n",
                "        else:\n",
                "            return \"disgust\"\n",
                "    return \"neutral\"\n",
                "\n",
                "def predict_audio_emotion(audio_path):\n",
                "    \"\"\"Predict emotion from audio file.\"\"\"\n",
                "    audio, sr = librosa.load(audio_path, sr=16000)\n",
                "    inputs = processor(audio, sampling_rate=16000)\n",
                "    input_values = torch.tensor(inputs.input_values).reshape(1, -1)\n",
                "    if torch.cuda.is_available():\n",
                "        input_values = input_values.cuda()\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        _, logits = audio_model(input_values)\n",
                "        dims = logits[0].cpu().numpy()\n",
                "    \n",
                "    arousal, dominance, valence = float(dims[0]), float(dims[1]), float(dims[2])\n",
                "    emotion = dimensions_to_emotion(arousal, valence, dominance)\n",
                "    \n",
                "    return {\n",
                "        'emotion': emotion,\n",
                "        'arousal': arousal,\n",
                "        'dominance': dominance,\n",
                "        'valence': valence\n",
                "    }"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Test on Audio Sample"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 31,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Audio Emotion: SADNESS\n",
                        "Dimensions: Arousal=0.01, Valence=-0.01, Dominance=0.05\n"
                    ]
                }
            ],
            "source": [
                "# Test with sample audio (update path as needed)\n",
                "audio_path = \"../Voice-Generation/generated_audio/hard_overlap.mp3\"\n",
                "\n",
                "if os.path.exists(audio_path):\n",
                "    result = predict_audio_emotion(audio_path)\n",
                "    print(f\"Audio Emotion: {result['emotion'].upper()}\")\n",
                "    print(f\"Dimensions: Arousal={result['arousal']:.2f}, Valence={result['valence']:.2f}, Dominance={result['dominance']:.2f}\")\n",
                "else:\n",
                "    print(f\"Audio file not found: {audio_path}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Summary\n",
                "\n",
                "| Model | Task | Output |\n",
                "|-------|------|--------|\n",
                "| `j-hartmann/emotion-english-distilroberta-base` | Text | 7 emotion classes |\n",
                "| `audeering/wav2vec2-large-robust-12-ft-emotion-msp-dim` | Audio | Valence/Arousal/Dominance → mapped to emotions |"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
